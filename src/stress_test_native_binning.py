import polars as pl
import numpy as np
import time
import psutil
import os
import gc
from typing import Tuple

# è°ƒæ•´å¯¼å…¥è·¯å¾„ä»¥åŒ¹é…ä½ çš„é¡¹ç›®ç»“æ„
try:
    # æ³¨æ„ï¼šè¿™é‡Œå¼•ç”¨çš„æ˜¯æœ€æ–°çš„ binning æ¨¡å—
    from mars.feature.binning import MarsNativeBinner
    from mars.utils.logger import set_log_level, logger
except ImportError:
    import sys
    sys.path.append("./src")
    from mars.feature.binning import MarsNativeBinner
    from mars.utils.logger import set_log_level, logger

# ==========================================
# âš™ï¸ å‹æµ‹é…ç½®
# ==========================================
set_log_level("INFO")

N_ROWS = 200_000      # 20ä¸‡è¡Œ
N_COLS = 2_000         # 2000ç‰¹å¾ (å¤§å®½è¡¨)
SPECIAL_VAL = -999.0   # ç‰¹æ®Šå€¼
MISSING_VAL = -1.0     # ä¸šåŠ¡ç¼ºå¤±å€¼

def get_memory_usage() -> float:
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜å ç”¨ (MB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def generate_massive_data() -> Tuple[pl.DataFrame, np.ndarray]:
    """
    æé€Ÿç”Ÿæˆ 20ä¸‡ x 2000åˆ— æ•°æ® (ç›´æ¥æ“ä½œ Numpy å†…å­˜ï¼Œé¿å…å¾ªç¯)
    """
    logger.info(f"ğŸš€ [DataGen] Allocating {N_ROWS:,} rows x {N_COLS} columns...")
    t0 = time.time()
    
    # 1. ç›´æ¥ç”Ÿæˆå¤§çŸ©é˜µ (Float32 èŠ‚çœå†…å­˜)
    data_matrix = np.random.randn(N_ROWS, N_COLS).astype(np.float32)
    
    # 2. æ³¨å…¥ Special Values (-999) - éšæœº 5%
    flat_view = data_matrix.ravel()
    n_elements = flat_view.size
    n_special = int(n_elements * 0.05)
    indices_spec = np.random.choice(n_elements, n_special, replace=False)
    flat_view[indices_spec] = SPECIAL_VAL
    
    # 3. æ³¨å…¥ Missing Values (None/NaN) - éšæœº 5%
    n_missing = int(n_elements * 0.05)
    indices_miss = np.random.choice(n_elements, n_missing, replace=False)
    flat_view[indices_miss] = np.nan
    
    # 4. ç”Ÿæˆ Target
    prob = 1 / (1 + np.exp(-np.clip(data_matrix[:, 0], -10, 10)))
    y = (np.random.rand(N_ROWS) < prob).astype(int)
    
    # 5. è½¬ Polars (Zero-Copy è½¬æ¢)
    logger.info("ğŸ“¦ Wrapping into Polars DataFrame...")
    col_names = [f"f_{i}" for i in range(N_COLS)]
    df = pl.from_numpy(data_matrix, schema=col_names)
    
    logger.info(f"âœ… Data Ready in {time.time() - t0:.2f}s | Memory: {get_memory_usage():.2f} MB")
    return df, y

def test_method(df: pl.DataFrame, y: np.ndarray, method: str, desc: str):
    print("\n" + "-"*60)
    print(f"ğŸ§ª Testing Method: [{method.upper()}] - {desc}")
    print("-"*60)
    
    gc.collect()
    mem_start = get_memory_usage()
    
    # åˆå§‹åŒ–
    binner = MarsNativeBinner(
        method=method,
        n_bins=5,
        special_values=[SPECIAL_VAL],
        missing_values=[MISSING_VAL], 
        n_jobs=-1 
    )
    
    # --- Fit æµ‹è¯• ---
    t0 = time.time()
    binner.fit(df, y)
    t_fit = time.time() - t0
    mem_peak = get_memory_usage() - mem_start
    
    print(f"   â±ï¸  Fit Time:       {t_fit:.4f} s")
    print(f"   ğŸ’¾  Mem Overhead:   {mem_peak:.2f} MB")
    
    # æ‰“å°ä¸€äº›åˆ‡ç‚¹ä¿¡æ¯ç”¨äºéªŒè¯
    if method in ["cart", "uniform"]:
        cut_0 = binner.bin_cuts_.get("f_0", [])
        print(f"   ğŸ”  Cuts (f_0):     {cut_0}")

    # --- Transform æµ‹è¯• ---
    t1 = time.time()
    df_res = binner.transform(df)
    # å¼ºåˆ¶è®¡ç®—
    _ = df_res[f"f_0_bin"].value_counts()
    t_trans = time.time() - t1
    
    print(f"   ğŸš€  Transform Time: {t_trans:.4f} s")
    
    # --- éªŒè¯ ---
    counts = df_res["f_0_bin"].value_counts().sort("f_0_bin")
    print(f"\n   ğŸ§ Sample Distribution (f_0_bin):")
    print(counts.head(7))
    
    return t_fit, t_trans

def run_stress_test():
    # 1. æ•°æ®ç”Ÿæˆ
    df_train, y_train = generate_massive_data()
    
    print("\n" + "="*80)
    print(f"ğŸ”¥ MARS NATIVE BINNER STRESS TEST")
    print(f"ğŸ”¥ Dimensions: {N_ROWS:,} rows x {N_COLS} columns")
    print("="*80)
    
    # 2. æµ‹è¯• Quantile (åŸºå‡†çº¿)
    t_fit_q, t_trans_q = test_method(
        df_train, y_train, 
        "quantile", 
        "Pure Polars (Zero-Copy)"
    )

    # 3. æµ‹è¯• Uniform (ç­‰å®½åˆ†ç®± - æ–°å¢)
    # é¢„æœŸï¼šæ¯” Quantile æ›´å¿«ï¼Œå› ä¸º min/max è®¡ç®—æ¯” quantile æ’åºè¦å¿«å¾—å¤š
    t_fit_u, t_trans_u = test_method(
        df_train, y_train, 
        "uniform", 
        "Pure Polars (Min/Max)"
    )
    
    # 4. æµ‹è¯• Decision Tree (å¹¶è¡Œ)
    t_fit_cart, t_trans_cart = test_method(
        df_train, y_train, 
        "cart", 
        "Parallel Sklearn (n_jobs=-1)"
    )
    
    # 5. æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ† FINAL SCOREBOARD")
    print("="*80)
    print(f"{'Method':<15} | {'Fit Time':<15} | {'Transform Time':<15} | {'Note':<20}")
    print("-" * 75)
    print(f"{'Quantile':<15} | {t_fit_q:<15.4f} | {t_trans_q:<15.4f} | {'Sorting based'}")
    print(f"{'Uniform':<15} | {t_fit_u:<15.4f} | {t_trans_u:<15.4f} | {'Min/Max based'}")
    print(f"{'DT (Parallel)':<15} | {t_fit_cart:<15.4f} | {t_trans_cart:<15.4f} | {'Tree based'}")
    print("-" * 75)
    
    # æ˜¾å¼æ¸…ç†
    del df_train, y_train
    gc.collect()

if __name__ == "__main__":
    run_stress_test()