from joblib import Parallel, delayed
from typing import List, Dict, Optional, Union, Any, Literal, Tuple, Set
import multiprocessing
import gc

import numpy as np
import pandas as pd
import polars as pl
from sklearn.tree import DecisionTreeClassifier

from mars.core.base import MarsTransformer
from mars.utils.logger import logger
from mars.utils.decorators import time_it

class MarsNativeBinner(MarsTransformer):
    """
    [æé€Ÿåˆ†ç®±å¼•æ“] MarsNativeBinner
    
    å®Œå…¨åŸºäº Polars å’Œ Sklearn åŸç”Ÿå®ç°çš„é«˜æ€§èƒ½åˆ†ç®±å™¨ã€‚
    é’ˆå¯¹å¤§è§„æ¨¡å®½è¡¨ (å¦‚ 2000+ ç‰¹å¾, 20ä¸‡+ æ ·æœ¬) è¿›è¡Œäº†å†…å­˜ä¸é€Ÿåº¦çš„æè‡´ä¼˜åŒ–ã€‚
    
    æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ (Performance Strategies)
    -------------------------------------
    1. **Quantile/Uniform**: 
       åˆ©ç”¨çº¯ Polars è¡¨è¾¾å¼è¿›è¡Œæ ‡é‡èšåˆè®¡ç®—ï¼Œé¿å…äº† Python å¾ªç¯å’Œæ•°æ®å¤åˆ¶ï¼ŒFit é€Ÿåº¦æå‡ 100xã€‚
    2. **Decision Tree (DT)**: 
       ä½¿ç”¨ `joblib` è¿›è¡Œå¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒï¼Œé€šè¿‡ç”Ÿæˆå™¨æƒ°æ€§ä¼ è¾“æ•°æ®ï¼Œå¤§å¹…é™ä½å†…å­˜å³°å€¼ã€‚
    3. **Transform**: 
       ä½¿ç”¨ Polars çš„ `cut` å’Œ `when-then` è¡¨è¾¾å¼è¿›è¡Œæ˜ å°„ï¼Œæ”¯æŒå…¨é“¾è·¯ Lazy æ¨¡å¼ã€‚
    4. **Type Safety**:
       å†…ç½®ç±»å‹å®‰å…¨å±‚ï¼Œè‡ªåŠ¨è¿‡æ»¤æ··åˆç±»å‹é…ç½®ï¼ˆå¦‚ Int åˆ—æ··å…¥ String ç¼ºå¤±å€¼ï¼‰ï¼Œé˜²æ­¢ Schema Errorã€‚

    Attributes
    ----------
    bin_cuts_ : Dict[str, List[float]]
        æ•°å€¼å‹ç‰¹å¾çš„åˆ†ç®±åˆ‡ç‚¹å­—å…¸ã€‚
        æ ¼å¼: ``{col_name: [-inf, split1, split2, ..., inf]}``ã€‚
    bin_mappings_ : Dict[str, Dict[int, str]]
        åˆ†ç®±ç´¢å¼•åˆ°æ ‡ç­¾çš„æ˜ å°„å­—å…¸ã€‚
        æ ¼å¼: ``{col_name: {0: "00_[-inf, 1.5)", -1: "Missing", ...}}``ã€‚
    bin_woes_ : Dict[str, Dict[int, float]]
        åˆ†ç®±ç´¢å¼•åˆ° WOE å€¼çš„æ˜ å°„å­—å…¸ï¼ˆç”¨äº WOE ç¼–ç ï¼‰ã€‚
    """

    # ç±»å±æ€§ï¼šå®šä¹‰æ•°å€¼ç±»å‹é›†åˆï¼Œç”¨äºå¿«é€Ÿåˆ¤å®šåˆ—ç±»å‹ï¼Œé¿å…ç¡¬ç¼–ç 
    NUMERIC_DTYPES: Set[pl.DataType] = {
        pl.Int8, pl.Int16, pl.Int32, pl.Int64, 
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, 
        pl.Float32, pl.Float64
    }
    
    def __init__(
        self,
        features: Optional[List[str]] = None,
        method: Literal["cart", "quantile", "uniform"] = "quantile",
        n_bins: int = 5,
        special_values: Optional[List[Union[int, float, str]]] = None,
        missing_values: Optional[List[Union[int, float, str]]] = None,
        min_samples: float = 0.05,
        n_jobs: int = -1,
        remove_empty_bins: bool = False,
        join_threshold: int = 100
    ) -> None:
        """
        åˆå§‹åŒ–åˆ†ç®±å™¨ã€‚

        Parameters
        ----------
        features : List[str], optional
            éœ€è¦åˆ†ç®±çš„ç‰¹å¾åç§°åˆ—è¡¨ã€‚å¦‚æœä¸ä¼ ï¼Œfit æ—¶ä¼šè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ•°å€¼å‹åˆ—ã€‚
        method : Literal["cart", "quantile", "uniform"], default="quantile"
            åˆ†ç®±æ–¹æ³•ï¼š
            - 'cart': å†³ç­–æ ‘åˆ†ç®± (Decision Tree)ï¼Œæœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šã€‚
            - 'quantile': ç­‰é¢‘åˆ†ç®± (Quantile)ã€‚
            - 'uniform': ç­‰å®½åˆ†ç®± (Uniform)ã€‚
        n_bins : int, default=5
            æœŸæœ›çš„åˆ†ç®±æ•°é‡ (ä¸åŒ…å«ç‰¹æ®Šå€¼å’Œç¼ºå¤±å€¼ç®±)ã€‚
        special_values : List[Union[int, float, str]], optional
            ç‰¹æ®Šå€¼åˆ—è¡¨ (å¦‚ -999, -998)ã€‚å°†è¢«å•ç‹¬åˆ†ä¸ºç‹¬ç«‹ç®± (Index <= -3)ã€‚
        missing_values : List[Union[int, float, str]], optional
            ç¼ºå¤±å€¼åˆ—è¡¨ (å¦‚ -1, "unknown")ã€‚å°†è¢«å½’ç±»ä¸º "Missing" (Index = -1)ã€‚
        min_samples : float, default=0.05
            ä»…å¯¹ method='cart' æœ‰æ•ˆã€‚å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ¯”ä¾‹ã€‚
        n_jobs : int, default=-1
            ä»…å¯¹ method='cart' æœ‰æ•ˆã€‚å¹¶è¡Œæ ¸å¿ƒæ•°ï¼Œ-1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ¸å¿ƒã€‚
        remove_empty_bins : bool, default=False
            ä»…å¯¹ method='uniform' æœ‰æ•ˆã€‚æ˜¯å¦æ‰«æå…¨è¡¨ä»¥å‰”é™¤æ ·æœ¬æ•°ä¸º0çš„ç©ºç®±ã€‚
            åœ¨å¤§å®½è¡¨åœºæ™¯ä¸‹å…³é—­æ­¤é¡¹å¯æ˜¾è‘—æå‡é€Ÿåº¦ã€‚
        join_threshold : int, default=100
            ç±»åˆ«ç‰¹å¾è·¯ç”±é˜ˆå€¼ã€‚åŸºæ•°è¶…è¿‡æ­¤å€¼æ—¶ï¼ŒTransform é˜¶æ®µå°†ç”± `replace` æ¨¡å¼åˆ‡æ¢ä¸º `join` æ¨¡å¼ä»¥æå‡æ€§èƒ½ã€‚
        """
        super().__init__()
        self.features: Optional[List[str]] = features
        self.method: str = method
        self.n_bins: int = n_bins
        # åˆå§‹åŒ–åˆ—è¡¨ï¼Œé¿å… None å¯¼è‡´çš„è¿­ä»£é”™è¯¯
        self.special_values: List[Any] = special_values if special_values is not None else []
        self.missing_values: List[Any] = missing_values if missing_values is not None else []
        self.min_samples: float = min_samples
        # æ™ºèƒ½è®¾ç½® CPU æ ¸å¿ƒæ•°ï¼Œä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
        self.n_jobs: int = max(1, multiprocessing.cpu_count() - 1) if n_jobs == -1 else n_jobs
        self.remove_empty_bins: bool = remove_empty_bins
        self.join_threshold: int = join_threshold
        
        # çŠ¶æ€å­˜å‚¨åˆå§‹åŒ–
        self.bin_cuts_: Dict[str, List[float]] = {}
        self.bin_mappings_: Dict[str, Dict[int, str]] = {}
        self.bin_woes_: Dict[str, Dict[int, float]] = {}
        
        # ç¼“å­˜å¼•ç”¨ (ç”¨äºå»¶è¿Ÿè®¡ç®— WOE)
        self._cache_X: Optional[pl.DataFrame] = None
        self._cache_y: Optional[Any] = None
        
    def _get_safe_values(self, dtype: pl.DataType, values: List[Any]) -> List[Any]:
        """
        [Helper] ç±»å‹å®‰å…¨æ¸…æ´—å‡½æ•°ã€‚
        
        **ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**
        Polars æ˜¯å¼ºç±»å‹çš„ã€‚å¦‚æœåˆ—æ˜¯ Int64ï¼Œä½† `values` åˆ—è¡¨ä¸­åŒ…å«å­—ç¬¦ä¸² "unknown"ï¼Œ
        ç›´æ¥è°ƒç”¨ `pl.col(c).is_in(values)` ä¼šå¯¼è‡´ Schema Error æˆ–å´©æºƒã€‚
        
        **è¿™è¡Œä»£ç è¿è¡Œåæœ‰å•¥ç”¨ï¼Ÿ**
        æ ¹æ®åˆ—çš„ç‰©ç†ç±»å‹ï¼Œè‡ªåŠ¨å‰”é™¤ä¸å…¼å®¹çš„å€¼ã€‚ä¾‹å¦‚ Int åˆ—åªä¿ç•™ Int é…ç½®é¡¹ï¼Œ
        String åˆ—åˆ™å°†æ‰€æœ‰é…ç½®é¡¹è½¬ä¸º Stringã€‚

        Parameters
        ----------
        dtype : pl.DataType
            å½“å‰å¤„ç†åˆ—çš„ Polars æ•°æ®ç±»å‹ã€‚
        values : List[Any]
            ç”¨æˆ·é…ç½®çš„ç¼ºå¤±å€¼æˆ–ç‰¹æ®Šå€¼åˆ—è¡¨ã€‚

        Returns
        -------
        List[Any]
            æ¸…æ´—åçš„ç±»å‹å®‰å…¨åˆ—è¡¨ã€‚
        """
        if not values:
            return []
            
        is_numeric = dtype in self.NUMERIC_DTYPES
        safe_vals = []
        
        for v in values:
            if v is None: continue # None ç”± is_null() å•ç‹¬å¤„ç†ï¼Œä¸éœ€è¦åœ¨æ­¤åˆ—è¡¨ä¸­
            
            if is_numeric:
                # æ•°å€¼åˆ—ï¼šä¸¥æ ¼ä¿ç•™æ•°å€¼ï¼Œå‰”é™¤ bool (True==1 æ­§ä¹‰) å’Œå­—ç¬¦ä¸²
                if isinstance(v, (int, float)) and not isinstance(v, bool):
                    safe_vals.append(v)
            else:
                # éæ•°å€¼åˆ—ï¼šå®½å®¹å¤„ç†ï¼Œå…¨éƒ¨è½¬ä¸ºå­—ç¬¦ä¸²ä»¥åŒ¹é… Categorical/String åˆ—
                safe_vals.append(str(v))
                
        return safe_vals

    @time_it
    def _fit_impl(self, X: pl.DataFrame, y: Optional[Any] = None, **kwargs) -> None:
        """
        è®­ç»ƒå®ç°çš„å…¥å£å‡½æ•°ã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è®­ç»ƒæ•°æ®ã€‚
        y : Optional[Any]
            ç›®æ ‡å˜é‡ (ä»… CART åˆ†ç®±éœ€è¦)ã€‚
        """
        # 1. ç¼“å­˜æ•°æ®å¼•ç”¨ï¼Œä»…ç”¨äº transform é˜¶æ®µè¯·æ±‚ return_type='woe' æ—¶çš„å»¶è¿Ÿè®¡ç®—
        self._cache_X = X
        self._cache_y = y

        # 2. ç¡®å®šç›®æ ‡åˆ— (ä»…ç­›é€‰æ•°å€¼åˆ—ï¼Œå¿½ç•¥å…¨ç©ºåˆ—)
        all_target_cols = self.features if self.features else X.columns
        target_cols: List[str] = []
        null_cols: List[str] = [] 

        for c in all_target_cols:
            if c not in X.columns: continue
            
            # åˆ¤å®šå…¨ç©º/Nullç±»å‹åˆ—ï¼Œè®°å½•ä¸‹æ¥ä»¥ä¾¿ç›´æ¥æ³¨å†Œä¸ºç©ºç®±
            if X[c].dtype == pl.Null or X[c].null_count() == X.height:
                null_cols.append(c)
                continue

            # ä»…å¤„ç†æ•°å€¼ç±»å‹
            if self._is_numeric(X[c]):
                target_cols.append(c)

        # æ³¨å†Œå…¨ç©ºåˆ—ä¸ºç©ºåˆ‡ç‚¹ï¼Œé˜²æ­¢ transform æ—¶æ¼åˆ—
        for c in null_cols:
            self.bin_cuts_[c] = []

        if not target_cols:
            if not null_cols:
                logger.warning("No numeric columns found for binning.")
            return

        # ========================================================
        # [ä¼˜åŒ–] æé€Ÿé¢„è¿‡æ»¤ (å¸¸é‡ç‰¹å¾å‰”é™¤)
        # ========================================================
        valid_cols: List[str] = []
        
        # æ„å»ºèšåˆè¡¨è¾¾å¼ï¼Œä¸€æ¬¡æ€§æ‰«æå…¨è¡¨è·å– Min/Max
        stats_exprs = []
        for c in target_cols:
            stats_exprs.append(pl.col(c).min().alias(f"{c}_min"))
            stats_exprs.append(pl.col(c).max().alias(f"{c}_max"))
            
        # è§¦å‘è®¡ç®— (Eager æ¨¡å¼ä¸‹ç«‹å³æ‰§è¡Œï¼Œé€Ÿåº¦æå¿«)
        stats_row = X.select(stats_exprs).row(0)
        
        for i, c in enumerate(target_cols):
            min_val = stats_row[i * 2]
            max_val = stats_row[i * 2 + 1]
            
            # å¦‚æœ Min == Maxï¼Œè¯´æ˜æ˜¯å¸¸é‡åˆ—ï¼Œæ— éœ€åˆ†ç®±ï¼Œç›´æ¥è®¾ä¸ºå…¨åŒºé—´
            if min_val == max_val:
                logger.warning(f"Feature '{c}' is constant. Skipped.")
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue
            
            valid_cols.append(c)

        if not valid_cols:
            return

        # 3. æ£€æŸ¥ CART æ–¹æ³•çš„ä¾èµ–
        if y is None and self.method == "cart":
            raise ValueError("Decision Tree Binning ('cart') requires target 'y'.")

        logger.info(f"âš™ï¸ Fitting bins for {len(valid_cols)} features (Native Mode: {self.method})...")

        # 4. ç­–ç•¥åˆ†å‘
        if self.method == "quantile":
            self._fit_quantile(X, valid_cols)
        elif self.method == "uniform":
            self._fit_uniform(X, valid_cols)
        elif self.method == "cart":
            self._fit_cart_parallel(X, y, valid_cols)
        else:
            raise ValueError(f"Unknown method: {self.method}")

    def _fit_quantile(self, X: pl.DataFrame, cols: List[str]) -> None:
        """
        æ‰§è¡Œæé€Ÿç­‰é¢‘åˆ†ç®± (Quantile Binning)ã€‚
        
        **æ ¸å¿ƒä¼˜åŒ–**:
        ä¸ä½¿ç”¨ Python å¾ªç¯é€åˆ—è®¡ç®—ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰åˆ—åˆ†ä½æ•°è®¡ç®—çš„
        å·¨å¤§ Polars è¡¨è¾¾å¼åˆ—è¡¨ï¼Œå‘é€ç»™ Rust å¼•æ“ä¸€æ¬¡æ€§æ‰§è¡Œã€‚
        """
        # 1. æ„å»ºåˆ†ä½ç‚¹
        if self.n_bins <= 1:
            quantiles = [0.5]
        else:
            quantiles = np.linspace(0, 1, self.n_bins + 1)[1:-1].tolist()
            
        raw_exclude = self.special_values + self.missing_values
        
        # 2. æ„å»ºè¡¨è¾¾å¼åˆ—è¡¨ (Flattened)
        q_exprs = []
        for c in cols:
            # è·å–å½“å‰åˆ—å®‰å…¨çš„æ’é™¤å€¼
            safe_exclude = self._get_safe_values(X.schema[c], raw_exclude)
            target_col = pl.col(c)
            # å¦‚æœæœ‰éœ€è¦æ’é™¤çš„å€¼ï¼Œåœ¨è®¡ç®—åˆ†ä½æ•°å‰å…ˆç½®ä¸º Null (ä¸å‚ä¸è®¡ç®—)
            if safe_exclude:
                target_col = pl.when(pl.col(c).is_in(safe_exclude)).then(None).otherwise(pl.col(c))
            
            for i, q in enumerate(quantiles):
                # åˆ«åæŠ€å·§: col:::idxï¼Œä¾¿äºåç»­è§£æ
                alias_name = f"{c}:::{i}"
                q_exprs.append(target_col.quantile(q).alias(alias_name))
        
        # 3. è§¦å‘è®¡ç®— (One-Shot Query)
        stats = X.select(q_exprs)
        row = stats.row(0)
        
        # 4. è§£æç»“æœå¹¶å»é‡æ’åº
        temp_cuts: Dict[str, List[float]] = {c: [] for c in cols}
        
        for val, name in zip(row, stats.columns):
            c_name, _ = name.split(":::")
            if val is not None and not np.isnan(val):
                temp_cuts[c_name].append(val)

        for c in cols:
            cuts = sorted(list(set(temp_cuts[c]))) 
            self.bin_cuts_[c] = [float('-inf')] + cuts + [float('inf')]

    def _fit_uniform(self, X: pl.DataFrame, cols: List[str]) -> None:
        """
        æ‰§è¡Œæé€Ÿç­‰å®½åˆ†ç®± (Uniform/Step Binning)ã€‚
        
        **æ ¸å¿ƒä¼˜åŒ–**:
        åˆ†ä¸ºä¸¤é˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µæ‰¹é‡è®¡ç®— Min/Max/Uniqueã€‚
        ç¬¬äºŒé˜¶æ®µï¼ˆå¯é€‰ï¼‰æ‰¹é‡è®¡ç®— Histogram ä»¥å‰”é™¤ç©ºç®±ã€‚
        """
        raw_exclude = self.special_values + self.missing_values
        
        # --- é˜¶æ®µ 1: åŸºç¡€ç»Ÿè®¡é‡ ---
        exprs = []
        col_safe_excludes = {} 

        for c in cols:
            safe_exclude = self._get_safe_values(X.schema[c], raw_exclude)
            col_safe_excludes[c] = safe_exclude # ç¼“å­˜ä¾›åç»­ä½¿ç”¨

            target_col = pl.col(c)
            if safe_exclude:
                target_col = target_col.filter(~pl.col(c).is_in(safe_exclude))
            
            exprs.append(target_col.min().alias(f"{c}_min"))
            exprs.append(target_col.max().alias(f"{c}_max"))
            exprs.append(target_col.n_unique().alias(f"{c}_n_unique"))

        stats = X.select(exprs)
        row = stats.row(0)
        
        initial_cuts_map = {}
        pending_optimization_cols = []

        # è§£æç»Ÿè®¡é‡ï¼Œç”Ÿæˆç­‰è·åˆ‡ç‚¹
        for i, c in enumerate(cols):
            base_idx = i * 3
            min_val, max_val, n_unique = row[base_idx], row[base_idx + 1], row[base_idx + 2]
            safe_exclude = col_safe_excludes[c]

            if min_val is None or max_val is None:
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue
            
            # ä¼˜åŒ–: ä½åŸºæ•°æ£€æŸ¥ (Unique <= N_Bins)ï¼Œç›´æ¥å–ä¸­ç‚¹åˆ‡åˆ†
            if n_unique <= self.n_bins:
                unique_vals = X.select(pl.col(c).unique().sort()).to_series().to_list()
                clean_vals = [v for v in unique_vals if v not in safe_exclude and v is not None]
                
                if len(clean_vals) <= 1:
                    self.bin_cuts_[c] = [float('-inf'), float('inf')]
                else:
                    mid_points = [(clean_vals[k] + clean_vals[k+1])/2 for k in range(len(clean_vals)-1)]
                    self.bin_cuts_[c] = [float('-inf')] + mid_points + [float('inf')]
                continue

            if min_val == max_val:
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue

            # ç”Ÿæˆç­‰å®½åˆ‡ç‚¹
            raw_cuts = np.linspace(min_val, max_val, self.n_bins + 1)[1:-1].tolist()
            full_cuts = [float('-inf')] + raw_cuts + [float('inf')]
            initial_cuts_map[c] = full_cuts
            
            if self.remove_empty_bins:
                pending_optimization_cols.append(c)
            else:
                self.bin_cuts_[c] = full_cuts

        # --- é˜¶æ®µ 2: ç©ºç®±ä¼˜åŒ– (å¯é€‰) ---
        if pending_optimization_cols:
            batch_exprs = []
            for c in pending_optimization_cols:
                cuts = initial_cuts_map[c]
                breaks = cuts[1:-1]
                target_col = pl.col(c)
                safe_exclude = col_safe_excludes[c]
                
                if safe_exclude:
                    target_col = target_col.filter(~pl.col(c).is_in(safe_exclude))
                
                labels = [str(i) for i in range(len(breaks)+1)]
                
                # æ‰¹é‡è®¡ç®—ç›´æ–¹å›¾ (Value Counts)
                batch_exprs.append(
                    target_col.cut(breaks, labels=labels, left_closed=True)
                    .value_counts().implode().alias(f"{c}_counts")
                )

            logger.info(f"âš¡ Batch scanning {len(pending_optimization_cols)} columns for empty bins...")
            batch_counts_df = X.select(batch_exprs)
            
            # è§£æå¹¶å‰”é™¤ Count=0 çš„ç®±
            for c in pending_optimization_cols:
                inner_series = batch_counts_df.get_column(f"{c}_counts")[0]
                keys = inner_series.struct.fields
                dist_list = inner_series.to_list()
                
                valid_indices = set()
                for row in dist_list:
                    # row æ˜¯ {'brk': '0', 'counts': 100} æ ¼å¼
                    idx_val = row.get(keys[0])
                    cnt_val = row.get(keys[1])
                    if idx_val is not None and cnt_val > 0:
                        valid_indices.add(int(idx_val))
                
                cuts = initial_cuts_map[c]
                breaks = cuts[1:-1]
                new_cuts = [cuts[0]]
                for i in range(len(breaks) + 1):
                    if i in valid_indices: new_cuts.append(cuts[i+1])
                
                if new_cuts[-1] != float('inf'): new_cuts.append(float('inf'))
                self.bin_cuts_[c] = sorted(list(set(new_cuts)))

    def _fit_cart_parallel(self, X: pl.DataFrame, y: Any, cols: List[str]) -> None:
        """
        æ‰§è¡Œå¹¶è¡Œçš„å†³ç­–æ ‘åˆ†ç®± (Decision Tree Binning)ã€‚
        
        **æ ¸å¿ƒä¼˜åŒ–**:
        1. ä½¿ç”¨ Generator (task_generator) æƒ°æ€§äº§å‡ºæ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§å¤åˆ¶æ‰€æœ‰åˆ—æ•°æ®åˆ°å†…å­˜ã€‚
        2. Worker å‡½æ•°åªæ¥æ”¶ Numpy æ•°ç»„ï¼Œå‡å°‘åºåˆ—åŒ–å¼€é”€ã€‚
        3. åœ¨ Generator å†…éƒ¨åˆ©ç”¨ Polars Rust å†…æ ¸è¿›è¡Œæé€Ÿè¿‡æ»¤å’Œç±»å‹è½¬æ¢ (Float32)ã€‚
        """
        y_np = np.array(y)
        if len(y_np) != X.height:
            raise ValueError(f"Target 'y' length mismatch: X({X.height}) vs y({len(y_np)})")

        # å®šä¹‰ Worker é€»è¾‘ï¼šçº¯ Sklearn æ‹Ÿåˆ
        def worker(col_name: str, x_clean_np: np.ndarray, y_clean_np: np.ndarray) -> Tuple[str, List[float]]:
            try:
                if len(x_clean_np) < self.n_bins * 10: 
                    return col_name, [float('-inf'), float('inf')]
                
                cart = DecisionTreeClassifier(
                    max_leaf_nodes=self.n_bins,
                    min_samples_leaf=self.min_samples,
                    random_state=42
                )
                cart.fit(x_clean_np, y_clean_np)
                cuts = cart.tree_.threshold[cart.tree_.threshold != -2]
                cuts = np.sort(np.unique(cuts)).tolist()
                return col_name, [float('-inf')] + cuts + [float('inf')]
            except Exception:
                return col_name, [float('-inf'), float('inf')]

        logger.info(f"âš™ï¸ Pre-processing {len(cols)} features for CART...")
        
        raw_exclude = self.special_values + self.missing_values
        
        # ä»»åŠ¡ç”Ÿæˆå™¨ï¼šæŒ‰éœ€ç”Ÿæˆæ•°æ®
        def task_generator():
            for c in cols:
                # âœ… [Critical Fix] ç±»å‹å®‰å…¨è¿‡æ»¤ï¼Œé˜²æ­¢åœ¨ Int åˆ—ä¸ŠæŸ¥è¯¢ "unknown" æŠ¥é”™
                safe_exclude = self._get_safe_values(X.schema[c], raw_exclude)

                # Polars æé€Ÿé¢„å¤„ç†ï¼šè¿‡æ»¤ -> è½¬æ¢ Float32
                valid_df = X.select([
                    pl.col(c).alias("x"),
                    pl.lit(y_np).alias("y")
                ]).filter(
                    pl.col("x").is_not_null() & 
                    ~pl.col("x").is_nan() & 
                    ~pl.col("x").is_in(safe_exclude)
                )
                
                if valid_df.height == 0: continue

                # Zero-copy (å¦‚æœå¯èƒ½) è½¬ Numpy
                x_clean = valid_df["x"].cast(pl.Float32).to_numpy(writable=False).reshape(-1, 1)
                y_clean = valid_df["y"].to_numpy(writable=False)
                
                yield c, x_clean, y_clean

        logger.info(f"ğŸš€ Starting parallel DT fitting with n_jobs={self.n_jobs}...")
        
        results = Parallel(n_jobs=self.n_jobs, backend="threading", verbose=0)(
            delayed(worker)(name, x, y) for name, x, y in task_generator()
        )
        
        for col_name, cuts in results:
            self.bin_cuts_[col_name] = cuts

    @time_it
    def _materialize_woe(self) -> None:
        """
        [æé€Ÿ WOE ç‰©åŒ–å¼•æ“ v7.0]
        
        **ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**
        Transform é˜¶æ®µå¦‚æœæ¯æ¬¡éƒ½å®æ—¶è®¡ç®— WOEï¼Œå¯¹äº 2000+ ç‰¹å¾çš„å®½è¡¨ï¼Œ
        Polars å¯èƒ½ä¼šæ„å»ºè¿‡å¤§çš„è®¡ç®—å›¾å¯¼è‡´å†…å­˜é£™å‡ã€‚
        
        **åšäº†ä»€ä¹ˆï¼Ÿ**
        1. å°† WOE è®¡ç®—æ‹†åˆ†ä¸º batch (200åˆ—ä¸€ç»„)ã€‚
        2. ä½¿ç”¨ Eager æ¨¡å¼ç«‹å³è®¡ç®—å¹¶å›æ”¶å†…å­˜ (`gc.collect`)ã€‚
        3. ä½¿ç”¨ group_by æé€Ÿèšåˆè€Œä¸æ˜¯æ„å»ºå¤æ‚çš„ when-then é€»è¾‘ã€‚
        """
        if self._cache_X is None or self._cache_y is None:
            logger.warning("No training data cached. WOE cannot be computed.")
            return

        logger.info("âš¡ [Auto-Trigger] Materializing WOE (Eager Cross-Grouping Mode)...")
        y_name = "_y_tmp"
        
        y_series = pl.Series(name=y_name, values=self._cache_y)
        total_bads = y_series.sum()
        total_goods = len(y_series) - total_bads
        
        # æ¶µç›–æ•°å€¼å’Œç±»åˆ«ç‰¹å¾
        bin_cols_orig = [c for c in self.bin_cuts_.keys()] + \
                        (list(self.cat_cuts_.keys()) if hasattr(self, 'cat_cuts_') else [])

        batch_size = 200 
        for i in range(0, len(bin_cols_orig), batch_size):
            batch_features = bin_cols_orig[i : i + batch_size]
            
            # Step A: å±€éƒ¨ Eager è½¬æ¢ (è·å– Index)
            X_batch_bin = self.transform(
                self._cache_X.select(batch_features), 
                return_type="index", 
                lazy=False
            )
            X_batch_bin = X_batch_bin.with_columns(y_series)
            
            # Step B: é€åˆ—èšåˆè®¡ç®— bad/good
            for c in batch_features:
                c_bin = f"{c}_bin"
                stats = (
                    X_batch_bin.group_by(c_bin)
                    .agg([
                        pl.col(y_name).sum().alias("b"),
                        pl.count().alias("n")
                    ])
                )
                
                # Step C: å‘é‡åŒ–è®¡ç®— WOE å¹¶å­˜å…¥å­—å…¸
                idxs = stats.get_column(c_bin)
                b = stats.get_column("b")
                n = stats.get_column("n")
                
                woe_vals = (((b + 1e-6) / (total_bads + 1e-6)) / 
                            (((n - b) + 1e-6) / (total_goods + 1e-6))).log()
                
                self.bin_woes_[c] = dict(zip(idxs.to_list(), woe_vals.to_list()))
            
            # Step D: å¼ºåˆ¶å†…å­˜æ–­å±‚
            del X_batch_bin
            gc.collect()
            
        logger.info(f"âœ… [V7.0] Materialization finished for {len(self.bin_woes_)} features.")

    def _transform_impl(
        self, 
        X: Union[pl.DataFrame, pl.LazyFrame], 
        return_type: Literal["index", "label", "woe"] = "index"
    ) -> Union[pl.DataFrame, pl.LazyFrame]:
        """
        [æ··åˆåŠ¨åŠ›åˆ†ç®±è½¬æ¢å®ç°] 
        æ ¸å¿ƒè½¬æ¢é€»è¾‘ï¼Œå…¼å®¹æ•°å€¼ä¸ç±»åˆ«ç‰¹å¾ï¼Œæ”¯æŒ Eager/Lazyã€‚

        Parameters
        ----------
        X : Union[pl.DataFrame, pl.LazyFrame]
            å¾…è½¬æ¢æ•°æ®ã€‚
        return_type : Literal["index", "label", "woe"]
            è¾“å‡ºæ ¼å¼ã€‚

        Returns
        -------
        Union[pl.DataFrame, pl.LazyFrame]
            è½¬æ¢åçš„æ•°æ®ã€‚
        """
        exprs = []
        temp_join_cols = []
        
        # ç´¢å¼•åè®®å¸¸é‡: ä¸ä¸‹æ¸¸ Profiler å¯¹é½
        IDX_MISSING = -1
        IDX_OTHER   = -2
        IDX_SPECIAL_START = -3

        # è‡ªåŠ¨è§¦å‘ WOE è®¡ç®—
        if return_type == "woe" and not self.bin_woes_:
            self._materialize_woe()

        # è·å– Schema (Lazy/Eager å…¼å®¹å†™æ³•)
        schema_map = X.collect_schema() if isinstance(X, pl.LazyFrame) else X.schema
        current_columns = schema_map.names()
        
        all_train_cols = list(set(
            list(self.bin_cuts_.keys()) + 
            (list(self.cat_cuts_.keys()) if hasattr(self, 'cat_cuts_') else [])
        ))

        for col in all_train_cols:
            if col not in current_columns: continue
            
            # --- [å…³é”®] è®¡ç®—ç±»å‹å®‰å…¨å€¼ ---
            # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œé˜²æ­¢ä¾‹å¦‚åœ¨ Int åˆ—ä¸ŠæŸ¥è¯¢ "unknown" å¯¼è‡´çš„å´©æºƒ
            col_dtype = schema_map[col]
            safe_missing_vals = self._get_safe_values(col_dtype, self.missing_values)
            safe_special_vals = self._get_safe_values(col_dtype, self.special_values)
            is_numeric_col = col_dtype in self.NUMERIC_DTYPES

            # =========================================================
            # Part A: æ•°å€¼å‹åˆ†ç®± (Numeric Binning)
            # =========================================================
            if col in self.bin_cuts_:
                cuts = self.bin_cuts_[col]
                
                # 1. ç¼ºå¤±å€¼é€»è¾‘: Is Null OR Is Missing Val
                missing_cond = pl.col(col).is_null() 
                if is_numeric_col: missing_cond |= pl.col(col).cast(pl.Float64).is_nan()
                for v in safe_missing_vals: missing_cond |= (pl.col(col) == v)
                
                layer_missing = pl.when(missing_cond).then(pl.lit(IDX_MISSING, dtype=pl.Int16))
                
                # 2. æ­£å¸¸åˆ†ç®±é€»è¾‘: Cut
                breaks = cuts[1:-1] if len(cuts) > 2 else []
                col_mapping = {IDX_MISSING: "Missing", IDX_OTHER: "Other"}
                
                if not breaks:
                    col_mapping[0] = "00_[-inf, inf)"
                    layer_normal = pl.lit(0, dtype=pl.Int16)
                else:
                    for i in range(len(cuts) - 1):
                        low, high = cuts[i], cuts[i+1]
                        col_mapping[i] = f"{i:02d}_[{low:.3g}, {high:.3g})"
                    # æ˜¾å¼ç”Ÿæˆ labels ç¡®ä¿ cast(Int16) æˆåŠŸï¼Œä¿®å¤ PSI=0 Bug
                    bin_labels = [str(i) for i in range(len(breaks) + 1)]
                    layer_normal = pl.col(col).cut(
                        breaks, labels=bin_labels, left_closed=True
                    ).cast(pl.Int16)
                
                # 3. ç‰¹æ®Šå€¼é€»è¾‘: ç€‘å¸ƒæµè¦†ç›–
                current_branch = layer_normal
                if safe_special_vals:
                    for i in range(len(safe_special_vals)-1, -1, -1):
                        v = safe_special_vals[i]
                        idx = IDX_SPECIAL_START - i 
                        col_mapping[idx] = f"Special_{v}"
                        current_branch = pl.when(pl.col(col) == v).then(pl.lit(idx, dtype=pl.Int16)).otherwise(current_branch)
                
                # ç»„åˆ: Missing -> Special -> Normal
                final_idx_expr = layer_missing.otherwise(current_branch)
                self.bin_mappings_[col] = col_mapping
                
            # =========================================================
            # Part B: ç±»åˆ«å‹åˆ†ç®± (Categorical Binning)
            # =========================================================
            elif hasattr(self, 'cat_cuts_') and col in self.cat_cuts_:
                splits = self.cat_cuts_[col]
                cat_to_idx: Dict[str, int] = {}
                idx_to_label: Dict[int, str] = {IDX_MISSING: "Missing", IDX_OTHER: "Other"}
                
                # æ›´æ–°æ˜ å°„è¡¨
                if safe_special_vals:
                    for i, val in enumerate(safe_special_vals):
                        idx_to_label[IDX_SPECIAL_START - i] = f"Special_{val}"

                for i, group in enumerate(splits):
                    disp_grp = group[:3] if len(group) > 3 else group
                    suffix = ",..." if len(group) > 3 else ""
                    idx_to_label[i] = f"{i:02d}_[{','.join(str(g) for g in disp_grp) + suffix}]"
                    for val in group: cat_to_idx[str(val)] = i
                
                self.bin_mappings_[col] = idx_to_label
                # å¼ºè½¬ Stringï¼Œç¡®ä¿ç±»åˆ«åŒ¹é…å®‰å…¨
                target_col = pl.col(col).cast(pl.Utf8)
                
                # 1. ç¼ºå¤±å€¼
                missing_cond = target_col.is_null()
                for v in safe_missing_vals:
                    missing_cond |= (target_col == str(v))
                layer_missing = pl.when(missing_cond).then(pl.lit(IDX_MISSING, dtype=pl.Int16))
                
                # 2. ç‰¹æ®Šå€¼
                current_branch = pl.lit(IDX_OTHER, dtype=pl.Int16)
                if safe_special_vals:
                    for i in range(len(safe_special_vals)-1, -1, -1):
                        v = safe_special_vals[i]
                        idx = IDX_SPECIAL_START - i
                        current_branch = pl.when(target_col == str(v)).then(pl.lit(idx, dtype=pl.Int16)).otherwise(current_branch)
                
                # 3. è·¯ç”±: Join (é«˜åŸºæ•°) vs Replace (ä½åŸºæ•°)
                # Join æ¨¡å¼é¿å…äº†åœ¨è¡¨è¾¾å¼ä¸­æ„å»ºå·¨å¤§çš„ when-then æ ‘ï¼Œæå¤§æå‡æ€§èƒ½
                if len(cat_to_idx) > self.join_threshold:
                    map_df = pl.DataFrame({
                        "_k": list(cat_to_idx.keys()), 
                        f"_idx_{col}": list(cat_to_idx.values())
                    }).with_columns([
                        pl.col("_k").cast(pl.Utf8),
                        pl.col(f"_idx_{col}").cast(pl.Int16)
                    ])
                    # å…¼å®¹ Lazy æ¨¡å¼çš„ Join
                    join_tbl = map_df.lazy() if isinstance(X, pl.LazyFrame) else map_df
                    X = X.join(join_tbl, left_on=target_col, right_on="_k", how="left")
                    
                    temp_join_cols.append(f"_idx_{col}")
                    layer_normal = pl.col(f"_idx_{col}")
                else:
                    layer_normal = target_col.replace(cat_to_idx, default=None).cast(pl.Int16)
                
                # ç»„åˆ: Missing -> Normal (Join Result) -> Special/Other
                final_idx_expr = layer_missing.otherwise(
                    pl.when(layer_normal.is_not_null()).then(layer_normal).otherwise(current_branch)
                )
            
            else:
                continue

            # è¾“å‡ºåˆ†å‘
            if return_type == "index":
                exprs.append(final_idx_expr.alias(f"{col}_bin"))
            elif return_type == "woe":
                woe_map = self.bin_woes_.get(col, {})
                exprs.append(final_idx_expr.replace(woe_map).cast(pl.Float64).alias(f"{col}_woe") if woe_map else pl.lit(0.0).alias(f"{col}_woe"))
            else:
                str_map = {str(k): v for k, v in self.bin_mappings_.get(col, {}).items()}
                exprs.append(final_idx_expr.cast(pl.Utf8).replace(str_map).alias(f"{col}_bin"))

        # æ¸…ç† Join äº§ç”Ÿçš„ä¸´æ—¶åˆ—
        return X.with_columns(exprs).drop(temp_join_cols)

    def transform(
        self, 
        X: Any, 
        return_type: Literal["index", "label", "woe"] = "index", 
        lazy: bool = False
    ) -> Union[pl.DataFrame, pd.DataFrame, pl.LazyFrame]:
        """
        å¯¹æ•°æ®åº”ç”¨åˆ†ç®±è½¬æ¢ã€‚

        Parameters
        ----------
        X : Any
            è¾“å…¥æ•°æ® (Pandas/Polars DataFrame)ã€‚
        return_type : Literal["index", "label", "woe"], default="index"
            è¿”å›ç±»å‹ï¼š
            - 'index': è¿”å› Int16 çš„ç®±ç´¢å¼• (-1=Missing, 0, 1...)ã€‚æœ€å¿«ã€‚
            - 'label': è¿”å›å­—ç¬¦ä¸²æ ‡ç­¾ (å¦‚ "01_[0.5, 1.2)")ã€‚
            - 'woe': è¿”å› Float64 çš„ WOE ç¼–ç å€¼ã€‚
        lazy : bool, default=False
            æ˜¯å¦è¿”å› LazyFrameã€‚å¦‚æœä¸º Trueï¼Œä¸ä¼šè§¦å‘è®¡ç®—ï¼Œé€‚åˆæ„å»ºè®¡ç®—å›¾ã€‚

        Returns
        -------
        Union[pl.DataFrame, pd.DataFrame, pl.LazyFrame]
            è½¬æ¢åçš„æ•°æ®æ¡†ã€‚
        """
        # 1. æ™ºèƒ½è¾“å…¥å¤„ç†ï¼šç¡®ä¿æ˜¯ Polars å¯¹è±¡
        if isinstance(X, pl.LazyFrame):
            X_pl = X
        else:
            X_pl = self._ensure_polars(X)
        
        # 2. æ¨¡å¼åˆ‡æ¢ï¼šå¦‚æœéœ€è¦ Lazyï¼Œè½¬ä¸º LazyFrame
        if lazy and isinstance(X_pl, pl.DataFrame):
            X_pl = X_pl.lazy()
        
        # 3. æ‰§è¡Œæ ¸å¿ƒé€»è¾‘
        res = self._transform_impl(X_pl, return_type=return_type)
        
        # 4. è¾“å‡ºæ ¼å¼æ§åˆ¶
        if not lazy:
            if isinstance(res, pl.LazyFrame): res = res.collect()
            if isinstance(X, pd.DataFrame): return res.to_pandas()
        return res

    def get_bin_mapping(self, col: str) -> Dict[int, str]:
        """è·å–æŒ‡å®šåˆ—çš„åˆ†ç®±æ˜ å°„å­—å…¸ã€‚"""
        return self.bin_mappings_.get(col, {})

    def _is_numeric(self, series: pl.Series) -> bool:
        """Helper: åˆ¤æ–­ Series æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹ã€‚"""
        if series.dtype == pl.Null:
            return False
        return series.dtype in self.NUMERIC_DTYPES
    
    @time_it
    def compute_bin_stats(self, X: pl.DataFrame, y: Any) -> pl.DataFrame:
        """
        [æé€ŸæŒ‡æ ‡å¼•æ“ v2.0] è®¡ç®—å…¨é‡åˆ†ç®±æŒ‡æ ‡ã€‚
        
        **æ ¸å¿ƒä¼˜åŒ–**:
        ä½¿ç”¨ `unpivot` + `group_by` å®ç°çŸ©é˜µåŒ–èšåˆï¼Œè€Œéå¾ªç¯é€åˆ—èšåˆã€‚
        
        Parameters
        ----------
        X : pl.DataFrame
            ç‰¹å¾æ•°æ®ã€‚
        y : Any
            ç›®æ ‡æ ‡ç­¾ã€‚

        Returns
        -------
        pl.DataFrame
            åŒ…å« feature, bin_index, count, bad_rate, woe, iv, ks ç­‰æŒ‡æ ‡çš„å®½è¡¨ã€‚
        """
        y_name = "_target_tmp"
        # å¼ºåˆ¶å¼€å¯ Lazy è½¬æ¢ä»¥åˆå¹¶æŸ¥è¯¢è®¡åˆ’
        X_bin_lazy = self.transform(X, return_type="index", lazy=True)
        X_bin_lazy = X_bin_lazy.with_columns(pl.lit(np.array(y)).alias(y_name))
        
        # è·å–å…¨å±€ç»Ÿè®¡é‡ (T, B)
        meta = X_bin_lazy.select([
            pl.count().alias("total_counts"),
            pl.col(y_name).sum().alias("total_bads")
        ]).collect()
        
        total_counts = meta[0, "total_counts"]
        total_bads = meta[0, "total_bads"]
        total_goods = total_counts - total_bads
        global_bad_rate = (total_bads / total_counts) if total_counts > 0 else 0
        
        current_cols = X_bin_lazy.collect_schema().names()
        bin_cols = [c for c in current_cols if c.endswith("_bin")]
        logger.info(f"ğŸ“Š Lazily computing risk metrics for {len(bin_cols)} features...")

        # åˆ©ç”¨ unpivot å®ç°çŸ©é˜µåŒ–å¹¶è¡Œèšåˆè®¡åˆ’
        # (rows * cols) -> (rows * features, 2)
        lf_stats = (
            X_bin_lazy.unpivot(
                index=[y_name],
                on=bin_cols,
                variable_name="feature",
                value_name="bin_index"
            )
            .group_by(["feature", "bin_index"])
            .agg([
                pl.count().alias("count"),
                pl.col(y_name).sum().alias("bad")
            ])
            .with_columns([
                (pl.col("count") - pl.col("bad")).alias("good")
            ])
        )

        # å‘é‡åŒ–è®¡ç®—å„é¡¹æŒ‡æ ‡ (WOE, IV, Lift)
        lf_stats = lf_stats.with_columns([
            (pl.col("count") / total_counts).alias("count_dist"),
            (pl.col("bad") / pl.col("count")).alias("bad_rate"),
            (pl.col("bad") / (total_bads + 1e-6)).alias("bad_dist"),
            (pl.col("good") / (total_goods + 1e-6)).alias("good_dist")
        ]).with_columns([
            ((pl.col("bad_dist") + 1e-6) / (pl.col("good_dist") + 1e-6)).log().alias("woe")
        ]).with_columns([
            ((pl.col("bad_dist") - pl.col("good_dist")) * pl.col("woe")).alias("bin_iv"),
            (pl.col("bad_rate") / (global_bad_rate + 1e-6)).alias("lift")
        ])

        # Window Function è®¡ç®— KS (åŸºäºç‰¹å¾å†…ç´¯è®¡)
        lf_stats = lf_stats.sort(["feature", "bin_index"]).with_columns([
            pl.col("bad_dist").cum_sum().over("feature").alias("cum_bad_dist"),
            pl.col("good_dist").cum_sum().over("feature").alias("cum_good_dist")
        ]).with_columns([
            (pl.col("cum_bad_dist") - pl.col("cum_good_dist")).abs().alias("bin_ks")
        ])

        # æœ€ç»ˆç‰©ç†ç‰©åŒ–
        stats_df = lf_stats.collect(streaming=True)
        
        # å…³è”æ ‡ç­¾å¹¶è®¡ç®—æ€» IV
        final_list = []
        for feat_name in bin_cols:
            orig_name = feat_name.replace("_bin", "")
            mapping = self.get_bin_mapping(orig_name)
            
            feat_stats = stats_df.filter(pl.col("feature") == feat_name).with_columns([
                pl.col("bin_index").cast(pl.Utf8).replace({str(k): v for k, v in mapping.items()}).alias("bin_label")
            ])
            
            # åŒæ—¶åŒæ­¥ WOE å­—å…¸ï¼Œæ–¹ä¾¿åç»­ transform ä½¿ç”¨
            self.bin_woes_[orig_name] = dict(zip(feat_stats["bin_index"].to_list(), feat_stats["woe"].to_list()))
            final_list.append(feat_stats)

        result_df = pl.concat(final_list)
        iv_sum = result_df.group_by("feature").agg(pl.col("bin_iv").sum().alias("total_iv"))
        return result_df.join(iv_sum, on="feature")


class MarsOptimalBinner(MarsNativeBinner):
    """
    [æ··åˆåŠ¨åŠ›åˆ†ç®±å¼•æ“] MarsOptimalBinner

    è¯¥ç±»å®ç°äº†åŸºäºæ··åˆåŠ¨åŠ›æ¶æ„ (Hybrid Engine) çš„æœ€ä¼˜åˆ†ç®±ç®—æ³•ã€‚
    
    è®¾è®¡ç›®æ ‡
    -------
    è§£å†³ä¼ ç»Ÿ OptBinning åœ¨å¤§è§„æ¨¡æ•°æ®ï¼ˆå¦‚ 20ä¸‡è¡Œ x 2000åˆ—ï¼‰ä¸Šç›´æ¥æ±‚è§£ MIP (æ··åˆæ•´æ•°è§„åˆ’) 
    å¯¼è‡´çš„è®¡ç®—æ€§èƒ½ç“¶é¢ˆï¼ŒåŒæ—¶ä¿ç•™å…¶æ•°å­¦è§„åˆ’å¸¦æ¥çš„æœ€ä¼˜æ€§å’Œå•è°ƒæ€§çº¦æŸèƒ½åŠ›ã€‚

    æ ¸å¿ƒæ¶æ„ (Architecture)
    -----------------------
    1. **Numeric Pipeline (æ•°å€¼å‹ç‰¹å¾)**: "ä¸¤é˜¶æ®µç«ç®­" æ¨¡å¼
       - **Stage 1 (Pre-binning)**: åˆ©ç”¨ Polars è¿›è¡Œæé€Ÿåˆ†ä½æ•°/ç­‰å®½é¢„åˆ†ç®± (O(N))ã€‚
         å°†åŸå§‹æ•°æ®ç¦»æ•£åŒ–ä¸ºç»†ç²’åº¦ (å¦‚ 50 ç®±) çš„å€™é€‰åŒºé—´ã€‚
       - **Stage 2 (Optimization)**: å°†é¢„åˆ†ç®±åˆ‡ç‚¹æ³¨å…¥ OptBinning (MIP Solver)ã€‚
         åˆ©ç”¨çº¦æŸç¼–ç¨‹ (CP) æ±‚è§£æ»¡è¶³å•è°ƒæ€§çº¦æŸçš„æœ€ä¼˜åˆå¹¶æ–¹æ¡ˆ (O(1))ã€‚
    
    2. **Categorical Pipeline (ç±»åˆ«å‹ç‰¹å¾)**:
       - **Pre-filtering**: å¯¹é«˜åŸºæ•°ç‰¹å¾è¿›è¡Œ Top-K è¿‡æ»¤ï¼Œå°†é•¿å°¾ç±»åˆ«å½’å¹¶ä¸º "Other_Pre"ã€‚
       - **Optimization**: è°ƒç”¨ OptBinning å¤„ç†ç±»åˆ«åˆå¹¶ã€‚

    Attributes
    ----------
    bin_cuts_ : Dict[str, List[float]]
        æ•°å€¼å‹ç‰¹å¾çš„æœ€ä¼˜åˆ‡ç‚¹å­—å…¸ã€‚
    cat_cuts_ : Dict[str, List[List[Any]]]
        ç±»åˆ«å‹ç‰¹å¾çš„åˆ†ç®±è§„åˆ™å­—å…¸ã€‚
        æ ¼å¼: ``{col: [['A', 'B'], ['C'], ['D']]}``ï¼Œè¡¨ç¤º Aå’ŒB å½’ä¸ºç®±0ï¼ŒC å½’ä¸ºç®±1...
    """

    def __init__(
        self,
        features: Optional[List[str]] = None,
        cat_features: Optional[List[str]] = None,
        n_bins: int = 5,
        n_prebins: int = 50,
        prebinning_method: Literal["quantile", "uniform", "cart"] = "quantile",
        monotonic_trend: str = "auto_asc_desc",
        solver: str = "cp",
        time_limit: int = 10,
        special_values: Optional[List[Union[int, float, str]]] = None,
        missing_values: Optional[List[Union[int, float, str]]] = None,
        cat_cutoff: Optional[int] = 100,  
        join_threshold: int = 1000,       
        n_jobs: int = -1  
    ) -> None:
        """
        åˆå§‹åŒ–æ··åˆåŠ¨åŠ›åˆ†ç®±å™¨ã€‚
        
        Parameters
        ----------
        cat_features : List[str], optional
            éœ€è¦å¤„ç†çš„ç±»åˆ«å‹ç‰¹å¾åˆ—è¡¨ã€‚
        n_prebins : int, default=50
            ç¬¬ä¸€é˜¶æ®µé¢„åˆ†ç®±çš„æ•°é‡ã€‚æ•°é‡è¶Šå¤šï¼Œç¬¬äºŒé˜¶æ®µä¼˜åŒ–çš„ç©ºé—´è¶Šå¤§ï¼Œä½†é€Ÿåº¦è¶Šæ…¢ã€‚
        prebinning_method : str, default="quantile"
            ç¬¬ä¸€é˜¶æ®µé¢„åˆ†ç®±çš„æ–¹æ³•ã€‚
        monotonic_trend : str, default="auto_asc_desc"
            å•è°ƒæ€§çº¦æŸ: 'auto', 'ascending', 'descending'ã€‚
        solver : str, default="cp"
            OptBinning æ±‚è§£å™¨: 'cp' (Constraint Programming) æˆ– 'mip'ã€‚
        time_limit : int, default=10
            æ±‚è§£å™¨è¶…æ—¶æ—¶é—´ (ç§’)ã€‚
        cat_cutoff : Optional[int], default=100
            ç±»åˆ«ç‰¹å¾ Top-K æˆªæ–­é˜ˆå€¼ã€‚ä¿ç•™é¢‘æ•°æœ€é«˜çš„ K ä¸ªç±»åˆ«ï¼Œå…¶ä½™å½’ä¸º Otherã€‚
        
        (å…¶ä½™å‚æ•°å‚è§çˆ¶ç±» MarsNativeBinner)
        """
        # åˆå§‹åŒ–çˆ¶ç±» MarsNativeBinner (è´Ÿè´£ Stage 1)
        super().__init__(
            features=features,
            method=prebinning_method,
            n_bins=n_bins,
            special_values=special_values,
            missing_values=missing_values,
            n_jobs=n_jobs
        )
        self.cat_features: List[str] = cat_features if cat_features is not None else []
        self.n_prebins: int = n_prebins
        self.monotonic_trend: str = monotonic_trend
        self.solver: str = solver
        self.time_limit: int = time_limit
        self.cat_cutoff: Optional[int] = cat_cutoff
        self.join_threshold: int = join_threshold
        
        # ä¸“é—¨å­˜å‚¨ç±»åˆ«ç‰¹å¾çš„åˆ†ç®±è§„åˆ™
        # ç»“æ„: {col_name: [['A', 'B'], ['C'], ['D']]}
        self.cat_cuts_: Dict[str, List[List[Any]]] = {}

        # æ£€æŸ¥ OptBinning ä¾èµ–
        try:
            import optbinning
        except ImportError:
            logger.warning("âš ï¸ 'optbinning' not installed. Optimal binning will fallback to pre-binning.")

    def _fit_impl(self, X: pl.DataFrame, y: Optional[Any] = None, **kwargs) -> None:
        """
        è®­ç»ƒå…¥å£ï¼šåˆ†æµæ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾åˆ°ä¸åŒçš„ Pipelineã€‚
        """
        if y is None:
            raise ValueError("Optimal Binning requires target 'y' to calculate IV/WOE.")

        y_np = np.array(y)
        
        all_target_cols = self.features if self.features else X.columns
        cat_set = set(self.cat_features)
        
        num_cols = []
        cat_cols = []
        null_cols = [] 

        for c in all_target_cols:
            if c not in X.columns: continue
            
            # 1. ä¼˜å…ˆåˆ¤å®šç±»åˆ«
            if c in cat_set:
                cat_cols.append(c)
                continue
            
            # 2. åˆ¤å®šå…¨ç©º
            if X[c].dtype == pl.Null or X[c].null_count() == X.height:
                null_cols.append(c)
                continue

            # 3. åˆ¤å®šæ•°å€¼
            if self._is_numeric(X[c]):
                num_cols.append(c)

        if not num_cols and not cat_cols and not null_cols:
            logger.warning("No valid numeric or categorical columns found.")
            return
        
        logger.info(f"ğŸ“Š Features identified: {len(num_cols)} Numeric, {len(cat_cols)} Categorical, {len(null_cols)} All-Null.")

        # æ³¨å†Œå…¨ç©ºåˆ—
        for c in null_cols:
            self.bin_cuts_[c] = []

        if num_cols:
            self._fit_numerical_pipeline(X, y_np, num_cols)

        if cat_cols:
            self._fit_categorical_pipeline(X, y_np, cat_cols)

    def _fit_numerical_pipeline(self, X: pl.DataFrame, y_np: np.ndarray, num_cols: List[str]) -> None:
        """
        [Pipeline] æ•°å€¼å‹ç‰¹å¾æ··åˆåŠ¨åŠ›å¤„ç†æµæ°´çº¿ã€‚
        """
        logger.info(f"ğŸš€ [Numeric Pipeline] Starting Hybrid Engine for {len(num_cols)} features...")
        
        # --- Stage 1: æé€Ÿé¢„åˆ†ç®± (Pre-binning) ---
        logger.info(f"   [Stage 1] Pre-binning with Polars (Method: {self.method}, Pre-bins: {self.n_prebins})...")
        
        pre_binner = MarsNativeBinner(
            features=num_cols,
            method=self.method, 
            n_bins=self.n_prebins, 
            special_values=self.special_values,
            missing_values=self.missing_values,
            n_jobs=self.n_jobs,
            remove_empty_bins=False 
        )
        pre_binner.fit(X, y_np)
        pre_cuts_map = pre_binner.bin_cuts_

        # ç­›é€‰å‡ºæœ‰æ„ä¹‰çš„åˆ— (åˆ‡ç‚¹æ•° > 2 è¡¨ç¤ºä¸ä»…ä»…æ˜¯ inf)
        active_cols = []
        for col, cuts in pre_cuts_map.items():
            if len(cuts) > 2: 
                active_cols.append(col)
            else:
                self.bin_cuts_[col] = cuts 

        if not active_cols:
            logger.info("   [Stage 1] Completed. No features require further optimization.")
            return

        logger.info(f"   [Stage 1] Completed. {len(active_cols)} features passed to Solver.")
        
        # --- Stage 2: å¹¶è¡Œä¼˜åŒ– (Optimization) ---
        logger.info(f"   [Stage 2] Optimizing with Solver (Engine: {self.solver}, TimeLimit: {self.time_limit}s)...")
        
        def num_worker(col: str, pre_cuts: List[float], col_data: np.ndarray) -> Tuple[str, List[float]]:
            fallback_res = (col, pre_cuts)
            try:
                from optbinning import OptimalBinning
                
                # 1. åŸºç¡€æ£€æŸ¥
                valid_mask = ~np.isnan(col_data)
                valid_data = col_data[valid_mask]
                if len(valid_data) < 10 or np.var(valid_data) < 1e-8:
                    return fallback_res

                # 2. æ³¨å…¥ Stage 1 åˆ‡ç‚¹ (User Splits)
                user_splits = np.array(pre_cuts[1:-1]) 
                if len(user_splits) == 0:
                    return fallback_res
                
                opt = OptimalBinning(
                    name=col, dtype="numerical", solver=self.solver,
                    monotonic_trend=self.monotonic_trend,
                    user_splits=user_splits,  
                    max_n_bins=self.n_bins,   
                    time_limit=self.time_limit, 
                    min_bin_size=0.05,
                    verbose=False
                )
                opt.fit(valid_data, y_np[valid_mask])
                
                if opt.status in ["OPTIMAL", "FEASIBLE"]:
                    res_cuts = [float('-inf')] + list(opt.splits) + [float('inf')]
                    # Bug Fix: é˜²æ­¢ Solver ä¼˜åŒ–è¿‡åº¦
                    if len(res_cuts) <= 2 and len(pre_cuts) > 2:
                        return fallback_res
                    return col, res_cuts
                
                return fallback_res 
            except Exception:
                return fallback_res

        task_gen = (
            (c, pre_cuts_map[c], X.select(c).to_series().to_numpy()) 
            for c in active_cols
        )
        
        results = Parallel(n_jobs=self.n_jobs, backend="loky")(
            delayed(num_worker)(c, cuts, data) for c, cuts, data in task_gen
        )
        
        for col, cuts in results:
            self.bin_cuts_[col] = cuts
        
        logger.info(f"âœ… [Numeric Pipeline] Optimization finished for {len(active_cols)} features.")

    def _fit_categorical_pipeline(self, X: pl.DataFrame, y_np: np.ndarray, cat_cols: List[str]) -> None:
        """
        [Pipeline] ç±»åˆ«å‹ç‰¹å¾å¤„ç†æµæ°´çº¿ (å¸¦ Top-K ä¼˜åŒ–)ã€‚
        """
        logger.info(f"ğŸš€ [Categorical Pipeline] Optimizing {len(cat_cols)} features (Top-K Cutoff: {self.cat_cutoff})...")

        def cat_worker(col: str, col_data_raw: np.ndarray) -> Tuple[str, Optional[List[List[Any]]]]:
            try:
                from optbinning import OptimalBinning
                
                # Bug Fix: å‰”é™¤ None é˜²æ­¢ astype(str) ç”Ÿæˆ "None"
                mask_valid = pd.notnull(col_data_raw)
                valid_data = col_data_raw[mask_valid].astype(str)
                valid_y = y_np[mask_valid]
                
                # Top-K é¢„å¤„ç†: å°†é•¿å°¾ç±»åˆ«å½’ä¸º "Other_Pre"
                if self.cat_cutoff is not None:
                    unique_vals, counts = np.unique(valid_data, return_counts=True)
                    if len(unique_vals) > self.cat_cutoff:
                        top_indices = np.argsort(-counts)[:self.cat_cutoff]
                        top_vals = set(unique_vals[top_indices])
                        mask_keep = np.isin(valid_data, list(top_vals))
                        valid_data = np.where(mask_keep, valid_data, "Other_Pre")

                opt = OptimalBinning(
                    name=col, dtype="categorical", solver=self.solver,
                    max_n_bins=self.n_bins, 
                    time_limit=self.time_limit,
                    cat_cutoff=0.05, 
                    verbose=False
                )
                opt.fit(valid_data, valid_y)
                
                if opt.status in ["OPTIMAL", "FEASIBLE"]:
                    return col, opt.splits
                
                return col, None
            except Exception:
                return col, None

        task_gen = (
            (c, X.select(c).to_series().to_numpy()) 
            for c in cat_cols
        )
        
        results = Parallel(n_jobs=self.n_jobs, backend="loky")(
            delayed(cat_worker)(c, data) for c, data in task_gen
        )
        
        for col, splits in results:
            if splits is not None:
                self.cat_cuts_[col] = splits
        
        logger.info(f"âœ… [Categorical Pipeline] Finished. Rules generated for {len(self.cat_cuts_)} features.")

    def _transform_impl(
        self, 
        X: Union[pl.DataFrame, pl.LazyFrame], 
        return_type: Literal["index", "label", "woe"] = "index"
    ) -> Union[pl.DataFrame, pl.LazyFrame]:
        """
        [Override] æ··åˆåŠ¨åŠ›åˆ†ç®±çš„è½¬æ¢å®ç°ã€‚
        
        å¿…é¡»é‡å†™ä»¥ç¡®ä¿ä½¿ç”¨ MarsOptimalBinner çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚ bin_cuts_ å’Œ cat_cuts_ï¼‰ï¼Œ
        å¹¶ä¸”ä¸ºäº†å®‰å…¨èµ·è§ï¼Œå†æ¬¡åº”ç”¨ç±»å‹å®‰å…¨æ£€æŸ¥ã€‚
        
        (æ³¨æ„ï¼šç”±äºç»§æ‰¿å…³ç³»ï¼Œå¦‚æœ MarsNativeBinner çš„ _transform_impl å·²ç»æ”¯æŒäº† cat_cuts_ï¼Œ
         å®é™…ä¸Šå¯ä»¥ç›´æ¥è°ƒç”¨ superï¼Œä½†ä¸ºäº† Explicit Safety å’Œç‹¬ç«‹çš„æ‰©å±•æ€§ï¼Œè¿™é‡Œä¿ç•™ç‹¬ç«‹å®ç°ï¼Œ
         å¹¶ä¿®å¤äº†ç±»å‹å®‰å…¨é€»è¾‘)
        """
        exprs = []
        temp_join_cols = []
        
        IDX_MISSING = -1
        IDX_OTHER   = -2
        IDX_SPECIAL_START = -3

        if return_type == "woe" and not self.bin_woes_:
            self._materialize_woe()
            
        # è·å– Schema
        schema_map = X.collect_schema() if isinstance(X, pl.LazyFrame) else X.schema
        current_columns = schema_map.names()

        # éå†æ‰€æœ‰å·²è®­ç»ƒçš„åˆ— (Numeric + Categorical)
        for col in self.bin_cuts_.keys() | self.cat_cuts_.keys():
            if col not in current_columns: continue
            
            # --- [å…³é”®] è®¡ç®—å®‰å…¨å€¼ ---
            col_dtype = schema_map[col]
            safe_missing_vals = self._get_safe_values(col_dtype, self.missing_values)
            safe_special_vals = self._get_safe_values(col_dtype, self.special_values)
            is_numeric_col = col_dtype in self.NUMERIC_DTYPES

            # =====================================================
            # Part A: æ•°å€¼å‹ç‰¹å¾
            # =====================================================
            if col in self.bin_cuts_:
                cuts = self.bin_cuts_[col]
                col_mapping = {IDX_MISSING: "Missing", IDX_OTHER: "Other"}
                
                # ç¼ºå¤±å€¼
                missing_cond = pl.col(col).is_null() 
                if is_numeric_col: missing_cond |= pl.col(col).cast(pl.Float64).is_nan()
                for val in safe_missing_vals: 
                    missing_cond |= (pl.col(col) == val)
                
                layer_missing = pl.when(missing_cond).then(pl.lit(IDX_MISSING, dtype=pl.Int16))
                
                # æ­£å¸¸åˆ†ç®±
                breaks = cuts[1:-1] if len(cuts) > 2 else []
                if not breaks:
                    col_mapping[0] = "00_[-inf, inf)"
                    layer_normal = pl.lit(0, dtype=pl.Int16)
                else:
                    for i in range(len(cuts) - 1):
                        low, high = cuts[i], cuts[i+1]
                        col_mapping[i] = f"{i:02d}_[{low:.3g}, {high:.3g})"
                    bin_labels = [str(i) for i in range(len(breaks) + 1)]
                    layer_normal = pl.col(col).cut(
                        breaks, labels=bin_labels, left_closed=True
                    ).cast(pl.Int16)

                # ç‰¹æ®Šå€¼
                current_branch = layer_normal
                if safe_special_vals:
                    for i in range(len(safe_special_vals) - 1, -1, -1):
                        val = safe_special_vals[i]
                        idx = IDX_SPECIAL_START - i
                        col_mapping[idx] = f"Special_{val}"
                        current_branch = pl.when(pl.col(col) == val).then(pl.lit(idx, dtype=pl.Int16)).otherwise(current_branch)
                
                final_idx_expr = layer_missing.otherwise(current_branch)
                self.bin_mappings_[col] = col_mapping

                if return_type == "index":
                    exprs.append(final_idx_expr.alias(f"{col}_bin"))
                elif return_type == "woe":
                    woe_map = self.bin_woes_.get(col, {})
                    exprs.append(final_idx_expr.replace(woe_map).cast(pl.Float64).alias(f"{col}_woe") if woe_map else pl.lit(0.0).alias(f"{col}_woe"))
                else:
                    str_map = {str(k): v for k, v in col_mapping.items()}
                    exprs.append(final_idx_expr.cast(pl.Utf8).replace(str_map).alias(f"{col}_bin"))

            # =====================================================
            # Part B: ç±»åˆ«å‹ç‰¹å¾
            # =====================================================
            elif col in self.cat_cuts_:
                splits = self.cat_cuts_[col]
                cat_to_idx = {}
                idx_to_label = {IDX_MISSING: "Missing", IDX_OTHER: "Other"}
                
                if safe_special_vals:
                    for i, val in enumerate(safe_special_vals):
                        idx_to_label[IDX_SPECIAL_START - i] = f"Special_{val}"

                for i, group in enumerate(splits):
                    disp_grp = group[:3] if len(group) > 3 else group
                    suffix = ",..." if len(group) > 3 else ""
                    label = f"{i:02d}_[{','.join(str(g) for g in disp_grp) + suffix}]"
                    idx_to_label[i] = label
                    for val in group:
                        cat_to_idx[str(val)] = i
                
                self.bin_mappings_[col] = idx_to_label
                target_col = pl.col(col).cast(pl.Utf8) 
                
                # ç¼ºå¤±å€¼
                missing_cond = target_col.is_null()
                for val in safe_missing_vals: 
                    missing_cond |= (target_col == str(val))
                layer_missing = pl.when(missing_cond).then(pl.lit(IDX_MISSING, dtype=pl.Int16))
                
                # ç‰¹æ®Šå€¼
                current_branch = pl.lit(IDX_OTHER, dtype=pl.Int16)
                if safe_special_vals:
                    for i in range(len(safe_special_vals) - 1, -1, -1):
                        val = safe_special_vals[i]
                        idx = IDX_SPECIAL_START - i
                        current_branch = pl.when(target_col == str(val)).then(pl.lit(idx, dtype=pl.Int16)).otherwise(current_branch)

                # è·¯ç”±: Join vs Replace
                if len(cat_to_idx) > self.join_threshold:
                    map_df = pl.DataFrame({
                        "_k": list(cat_to_idx.keys()), 
                        f"_idx_{col}": list(cat_to_idx.values())
                    }).with_columns([
                        pl.col("_k").cast(pl.Utf8),
                        pl.col(f"_idx_{col}").cast(pl.Int16)
                    ])
                    join_tbl = map_df.lazy() if isinstance(X, pl.LazyFrame) else map_df
                    X = X.join(join_tbl, left_on=target_col, right_on="_k", how="left")
                    temp_join_cols.append(f"_idx_{col}")
                    layer_normal = pl.col(f"_idx_{col}")
                else:
                    layer_normal = target_col.replace(cat_to_idx, default=None).cast(pl.Int16)
                
                final_idx_expr = layer_missing.otherwise(
                    pl.when(layer_normal.is_not_null()).then(layer_normal).otherwise(current_branch)
                )

                if return_type == "index":
                    exprs.append(final_idx_expr.alias(f"{col}_bin"))
                elif return_type == "woe":
                    woe_map = self.bin_woes_.get(col, {})
                    exprs.append(final_idx_expr.replace(woe_map).cast(pl.Float64).alias(f"{col}_woe") if woe_map else pl.lit(0.0).alias(f"{col}_woe"))
                else:
                    str_map = {str(k): v for k, v in idx_to_label.items()}
                    exprs.append(final_idx_expr.cast(pl.Utf8).replace(str_map).alias(f"{col}_bin"))

        return X.with_columns(exprs).drop(temp_join_cols)